%Version 3 December 2023
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove Numbered in the optional parenthesis.\\

%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst%

%%\documentclass[pdflatex,sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[pdflatex, sn-mathphys-num, lineno]{sn-jnl}% Math and Physical Sciences Numbered Reference Style
%%\documentclass[pdflatex,sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[pdflatex,sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[pdflatex,sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
%%\documentclass[pdflatex,sn-apa]{sn-jnl}% APA Reference Style
%%\documentclass[pdflatex,sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
%%%%

\usepackage{xspace}
\usepackage{standalone}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{positioning}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{decorations.pathreplacing}
\usepackage{listofitems} % for \readlist to create arrays

%% my package
\usepackage[detect-all=true]{siunitx}
\sisetup{
    scientific-notation=true,
    round-mode = places,
    round-precision = 2
}

\usepackage[acronym, automake, style=index, shortcuts]{glossaries-extra}
\setabbreviationstyle[acronym]{long-short}

% define glossaries
\makeglossaries
\newacronym{blat}{BLAT}{BLAST-like alignment tool}
\newacronym{llm}{LLM}{Large language model}
\newacronym{hmm}{HMM}{Hidden Markov Model}
\newacronym{gpu}{GPU}{Graphics Processing Unit}
\newacronym{drs}{DRS}{NanoPore direct RNA sequencing}
\newacronym{bert}{BERT}{Bidirectional Encoder Representations from Transformers}
\newacronym{gpt}{GPT}{Generative Pre-trained Transformer}

\newacronym{ide}{IDE}{Integrated Development Environment}
\newacronym{gil}{GIL}{Global Interpreter Lock}
\newacronym{ci}{CI}{Continuous Integration}
\newacronym{cd}{CD}{Continuous Development}
\newacronym{ucsc}{UCSC}{UCSC Genome Browser}

% define macros
% chopper = DeepChopper
\newcommand{\chopper}{ProjectX\xspace}

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published
%%%%  by Springer Nature. The guidance has been prepared in partnership with
%%%%  production teams to conform to Springer Nature technical requirements.
%%%%  Editorial and presentation requirements differ among journal portfolios and
%%%%  research disciplines. You may find sections in this template are irrelevant
%%%%  to your work and are empowered to omit any such section if allowed by the
%%%%  journal you intend to submit to. The submission guidelines and policies
%%%%  of the journal take precedence. A detailed User Manual is available in the
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}%
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[Article Title]{Large language models for long-context predictions in biological sequences}

%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg}
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%

\author[1]{\fnm{Yangyang} \sur{Li}}\email{yangyang.li@northwestern.edu}
\equalcont{These authors contributed equally to this work.}

% \author*[1,2]{\fnm{First} \sur{Author}}\email{iauthor@gmail.com}
\author[1]{\fnm{Tingyou} \sur{Wang}}\email{tingyou.wang@northwestern.edu}
\equalcont{These authors contributed equally to this work.}

\author*[1,2]{\fnm{Rendong} \sur{Yang}}\email{rendong.yang@northwestern.edu}

\affil[1]{\orgdiv{Department of Urology}, \orgname{Northwestern University Feinberg School of Medicine}, \orgaddress{\street{303 E Superior St}, \city{Chicago}, \postcode{60611}, \state{IL}, \country{USA}}}

\affil[2]{\orgdiv{Robert H. Lurie Comprehensive Cancer Center}, \orgname{Northwestern University Feinberg School of Medicine}, \orgaddress{\street{675 N St Clair St}, \city{Chicago}, \postcode{60611}, \state{IL}, \country{USA}}}

% \author[1,2]{\fnm{Third} \sur{Author}}\email{iiiauthor@gmail.com}
% \equalcont{These authors contributed equally to this work.}

% \affil*[1]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{100190}, \state{State}, \country{Country}}}

% \affil[2]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{10587}, \state{State}, \country{Country}}}

% \affil[3]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{610101}, \state{State}, \country{Country}}}

%%==================================%%
%% Sample for unstructured abstract %%
%%==================================%%

\abstract{
	\glspl{llm} have revolutionized natural language processing and are now poised to transform biological sequence analysis.
	This study explores the application of \glspl{llm} to \gls{drs} data, focusing on predicting adapters.
	Our results demonstrate that \glspl{llm} can effectively handle long contexts in biological sequences, providing accurate and efficient predictions.
	This showcases the potential for broader applications of \glspl{llm} in genomics and transcriptomics, offering a powerful tool for the biotechnology field.}

\maketitle

\section{Main}\label{sec1}

% Introduction

The advent of \glspl{llm} has revolutionized natural language processing, enabling unprecedented advancements in tasks requiring the comprehension of extensive contexts.
This breakthrough holds significant promise for the field of bioinformatics, where biological sequences often exhibit complex and lengthy patterns.
Here, we present \chopper, a novel application of \glspl{llm} in predicting adapters in \gls{drs} data, demonstrating the potential of these models to handle long-context biological sequences and their applicability to other problems such as m6A detection in long reads.

Nanopore sequencing, particularly direct RNA sequencing, provides a detailed view of RNA molecules, capturing a wide range of sequence information including modifications.
However, accurately identifying adapters within these sequences is a critical challenge that impacts the overall data quality and subsequent analysis.
Traditional methods often struggle with the intricacies of long RNA sequences, leading to errors and inefficiencies.

To address this, we developed \chopper, a language model tailored for biological sequences, leveraging the architecture and training paradigms of state-of-the-art \glspl{llm}.
Our model was trained on extensive \gls{drs} datasets, learning to predict artifitial sequences with high accuracy.
The application of such a model not only enhances the precision of artifitial sequences identification but also opens new avenues for analyzing other complex patterns in genomic data, such as m6A modifications and errorous base detection.

The core of our approach lies in the ability of \glspl{llm} to understand and generate sequences based on context.
By treating biological sequences as a form of language, the model captures the underlying biological grammar, enabling it to predict  artifitial sequences embedded within long sequences.
This method significantly outperforms traditional algorithms, particularly in handling the length and variability inherent in RNA sequences.

Our findings indicate that the \chopper can effectively parse through vast amounts of sequencing data, identifying  artifitial sequences with remarkable accuracy.
This capability is crucial for improving the overall quality of sequencing data and ensuring reliable downstream analyses.
Moreover, the flexibility of \glspl{llm} allows for adaptation to various other tasks within bioinformatics, such as m6A detection, gene prediction, variant calling, and functional annotation.

In conclusion, the integration of \glspl{llm} into bioinformatics presents a powerful tool for advancing genomic research.
By successfully predicting adapters in \gls{drs} data, we demonstrate the potential of \glspl{llm} to enhance the accuracy and efficiency of biological sequence analysis.
This study, exemplified by \chopper, paves the way for broader applications of \glspl{llm} in understanding and interpreting the vast complexities of genomic information.

Language models, particularly large-scale ones like \gls{gpt} and \gls{bert}~\cite{devlin2018bert}, have achieved remarkable success in various natural language processing tasks.
These models, by leveraging vast amounts of data and complex architectures, can understand and generate human-like text.
The principles underlying these models can be extended to biological sequences, where DNA, RNA, and protein sequences can be treated as languages with their own syntax and semantics.

\gls{drs} is a cutting-edge technology that enables the sequencing of RNA molecules directly, providing insights into transcriptomic landscapes.
However, accurately predicting adapter sequences, which are crucial for correct read alignment and downstream analysis, remains a significant challenge.
Traditional methods often struggle with the complexity and variability inherent in these sequences.

In this study, we explore the use of large language models to predict adapters in \gls{drs} data.
By leveraging the capacity of \glspl{llm} to handle large contexts, we aim to improve the accuracy and efficiency of adapter prediction, thereby enhancing the overall quality of sequencing data analysis.

% Importance of Language Models in Biological Sequences

Language models excel in understanding and generating sequences based on context, a capability that is highly relevant to biological data.
Biological sequences, much like natural languages, exhibit patterns and dependencies over long ranges, making them suitable candidates for language model applications.
Traditional methods of sequence analysis, such as \glspl{hmm} and alignment-based techniques, often face limitations in handling long-range dependencies and large datasets.
In contrast, \glspl{llm}, with their transformer-based architects, can process and learn from extensive sequence data, capturing intricate patterns and relationships.

For example, recent studies have demonstrated the effectiveness of language models in predicting protein structures and functions, understanding genomic variations, and annotating functional elements in genomes.
The ability to process large contexts is particularly advantageous in these applications, where local and global sequence features are crucial for accurate predictions.

In the context of \gls{drs}, the application of \glspl{llm} can address specific challenges such as predicting adapter sequences.
These sequences, which are essential for the correct alignment of reads, making accurate prediction difficult with traditional methods.
By leveraging the extensive contextual understanding provided by \glspl{llm}, we can improve the accuracy of these predictions, leading to better sequencing outcomes.

\begin{figure}
	\begin{center}
		\includestandalone[scale=0.3]{figures/model_arch}
	\end{center}
	\caption{Model Architecture}\label{fig:model}
\end{figure}

\begin{figure}[!h]
	%    \includegraphics[width=\linewidth]{figures/fas_len_ranges}
	\includegraphics[height=0.6\columnwidth]{example-image-duck}
	\caption{\bf This is figure 1.}
	\label{fig:f1}
\end{figure}

% Predicting Adapters in Nanopore Direct RNA Sequencing

\gls{drs} allows for real-time, high-throughput sequencing of RNA molecules, providing a comprehensive view of the transcriptome.
However, the presence of adapter sequences, necessary for sequencing, poses a challenge for data analysis.
Accurate prediction of these adapters is crucial for aligning reads and ensuring the reliability of downstream analyses.

In this study, we employed a large language model to predict adapter sequences in \gls{drs} data.
Our model was trained on a diverse dataset of RNA sequences, including various adapter types and configurations.
The training process involved fine-tuning the model on this specific task, optimizing for accuracy and efficiency.

The results demonstrate that our language model can predict adapters with high accuracy, outperforming traditional methods.
Figures 1 and 2 illustrate the comparison between our model's predictions and the actual adapter sequences, highlighting the model's precision and recall.

These findings suggest that \glspl{llm} can effectively handle the complexity and variability of biological sequences, making them a valuable tool for improving the accuracy of \gls{drs}.
This not only enhances the quality of sequencing data but also opens up new possibilities for the application of language models in other areas of genomics and transcriptomics.

% Conclusion

Our study demonstrates the potential of large language models in biological sequence analysis, particularly in the context of \gls{drs}.
By accurately predicting adapter sequences,  \glspl{llm} can significantly improve the quality and reliability of sequencing data.
This showcases the broader applicability of language models in handling complex biological data, paving the way for advancements in genomics and transcriptomics.
Future research can explore additional applications and further optimize these models for specific biological tasks, solidifying their role in biotechnology.

\begin{figure}[!h]
	\includegraphics[height=0.6\columnwidth]{example-image-duck}
	\caption{\bf This is figure 2.}
	\label{fig:f2}
\end{figure}

\section{Methods}\label{sec2}

\subsection{Language Model Architecture}

We adapted a transformer-based language model architecture, fine-tuning it for biological sequences.
The model was trained to predict  artifitial sequences based on the contextual information present in the RNA sequences.
The language model used in this study is based on the transformer architecture, which has been highly successful in natural language processing.
We design a qual block to use quality of the input sequence to predict output.
The backbone is used to conduct feature extration, and help us better understand the biological sequnces.
Specifically, we employed a model with \num{24} layers, \num{16} attention heads, and a hidden size of \num{1024}.
This configuration was chosen to balance performance and computational feasibility.
Training the model involved using a corpus of \gls{drs} sequences, including annotated ar regions.
The model was pretrained on a large dataset of general RNA sequences before fine-tuning on the specific task of adapter prediction.
This approach allowed the model to leverage general RNA sequence patterns while focusing on the nuances of adapter sequences during fine-tuning.

\subsection{Data Preparation}

\gls{drs} datasets were obtained from publicly available repositories, ensuring a comprehensive representation of diverse RNA sequences.
These datasets included both raw sequencing reads and annotated adapters.
The dataset for this study comprised publicly available \gls{drs} data, supplemented with manually annotated adapter sequences.
Preprocessing involved quality control steps such as read filtering and trimming to ensure high-quality data.
Adapter sequences were identified and labeled, creating a training set with positive (adapter) and negative (non-adapter) examples.
To further enhance the dataset, synthetic adapters were generated based on known sequences, providing additional training examples and improving the model's generalization capability.


\subsection{Training Procedure}

The model was trained using supervised learning, with sequences annotated for adapter regions serving as the training data.
Various data augmentation techniques were employed to enhance model robustness and generalization.
Training was conducted on a high-performance computing cluster, utilizing \glspl{gpu} to expedite the process.
The model was trained using the Adam optimization algorithm, with a learning rate of \num{1e-4}.
The training process involved multiple epochs, with early stopping criteria based on validation performance to prevent overfitting.
Evaluation metrics included accuracy, precision, recall, and F1 score, with cross-validation to ensure robustness.
The final model was selected based on the best performance on the validation set, and its predictions were compared against a baseline model using traditional methods.


We perform multi-task learning to find the optimal model’s parameters.
The total loss for one window is the sum of the losses from each supported position.
Total loss for supported position  \( i \), where \( i  \in {1,2, \ldots, S} \), is a sum of two losses: base prediction loss and informative position prediction loss and is given with :
where is the cross-entropy loss,  is the binary cross-entropy loss, represents the predicted base probabilitiesis the true base.
denotes the predicted probability that a
position is informative, and is the indicator function that equals to 1, and 0 otherwise.
We trained the model on 2 A100 GPUs for a maximum of 1 million steps.
The batch size was 128, so the model saw 128 million examples.
Validation was performed every 2000 steps, and we selected the model with the highest validation F1 score for the base prediction task.
The Adam optimizer was used for training with default \( \beta_{1} = 0.9 \) and \( \beta_{2} = 0.999 \) parameters.
We used a cosine learning rate scheduler with an initial learning rate of \( 3 \times 10^{-4} \).
Weight decay was set to 0.1 and gradients were clipped using the \( \ell^{2}\)-norm with a maximum norm value of 1.
The dropout value in transformer was set to \num{0.1}.
The complete list of hyperparameters can be found in Supplementary Table 11

\subsection{Evaluation Metrics and Analysis}

The evaluation metrics used to assess the model's performance included accuracy, precision, recall, and F1 score.
These metrics provided a comprehensive view of the model's ability to correctly identify adapter sequences while minimizing false positives and negatives.
Statistical analysis was conducted to compare the performance of our language model with traditional methods, using paired t-tests to determine the significance of improvements.
Additionally, the model's predictions were visualized using confusion matrices and precision-recall curves, highlighting its effectiveness in different scenarios.
The results were further validated by testing the model on an independent dataset, demonstrating its generalizability and robustness.

% \section{Tables}\label{sec5}

% Tables can be inserted via the normal table and tabular environment. To put
% footnotes inside tables you should use \verb+\footnotetext[]{...}+ tag.
% The footnote appears just below the table itself (refer Tables~\ref{tab1} and \ref{tab2}).
% For the corresponding footnotemark use \verb+\footnotemark[...]+

% \begin{table}[h]
% \caption{Caption text}\label{tab1}%
% \begin{tabular}{@{}llll@{}}
% \toprule
% Column 1 & Column 2  & Column 3 & Column 4\\
% \midrule
% row 1    & data 1   & data 2  & data 3  \\
% row 2    & data 4   & data 5\footnotemark[1]  & data 6  \\
% row 3    & data 7   & data 8  & data 9\footnotemark[2]  \\
% \botrule
% \end{tabular}

% \footnotetext{Source: This is an example of table footnote. This is an example of table footnote.}
% \footnotetext[1]{Example for a first table footnote. This is an example of table footnote.}
% \footnotetext[2]{Example for a second table footnote. This is an example of table footnote.}
% \end{table}

\begin{table}[h]
	\caption{Benchmarking for different models}
	\label{tab:bechmark}
	\begin{tabular}{@{}
			l
			S[table-format=1.4e-2] % Formats the F1 column in scientific notation
			S[table-format=1.4e-2] % Formats the Loss column in scientific notation
			@{}}
		\toprule
		{Model}             & {F1}                         & {Loss}                         \\ \midrule
		CNN                 & 0.9909037351608276           & 0.00302332011051476            \\
		Model with Hyean    & 0.9925663471221924           & 0.002182388212531805           \\
		Model with Caduceus & \bfseries 0.9977396130561829 & \bfseries 0.000541145505849272 \\ \bottomrule
	\end{tabular}
	% \footnotetext{Source: This is an example of table footnote. This is an example of table footnote.}
	% \footnotetext[1]{Example for a first table footnote. This is an example of table footnote.}
	% \footnotetext[2]{Example for a second table footnote. This is an example of table footnote.}
\end{table}


\backmatter

\bmhead{Supplementary information}

% If your article has accompanying supplementary file/s please state so here.
% Authors reporting data from electrophoretic gels and blots should supply the full unprocessed scans for key as part of their Supplementary information. This may be requested by the editorial team/s if it is missing.
% Please refer to Journal-level guidance for any specific requirements.
\bmhead{Data Availability}

The sources of HiFi, ONT and CLR reads of the six family datasets and HCC1395 normal-tumor paired cell are listed.
The human reference genome GRCh38 was downloaded from \url{http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/GRCh38\_reference\_genome/}.

\bmhead{Code Availability}

\chopper (v.1.0) is available at GitHub (\url{https://github.com/ylab-hi/DeepChopper}).
The scripts for model training, performance valuation and simulate data generation are available at GitHub (\url{https://github.com/ylab-hi/DeepChopper}).
Both repositories are available under a MIT License.

\bmhead{Acknowledgements}

Acknowledgements are not compulsory. Where included they should be brief. Grant or contribution numbers may be acknowledged.
Please refer to Journal-level guidance for any specific requirements.

% \section*{Declarations}

% Some journals require declarations to be submitted in a standardised format. Please check the Instructions for Authors of the journal to which you are submitting to see if you need to complete this section. If yes, your manuscript must contain the following sections under the heading `Declarations':

\begin{itemize}
	\item Funding
	\item Conflict of interest/Competing interests (check journal-specific guidelines for which heading to use)
	\item Ethics approval and consent to participate
	\item Consent for publication
	\item Data availability
	\item Materials availability
	\item Code availability
	\item Author contribution
\end{itemize}


\begin{appendices}


	\printglossary[type=\acronymtype, title=Abbreviations]


	\section{Section title of first appendix}\label{secA1}

	An appendix contains supplementary information that is not an essential part of the text itself but which may be helpful in providing a more comprehensive understanding of the research problem or it is information that is too cumbersome to be included in the body of the paper.

	%%=============================================%%
	%% For submissions to Nature Portfolio Journals%%
	%% please use the heading ``Extended Data''.   %%
	%%=============================================%%

	%%=============================================================%%
	%% Sample for another appendix section			               %%
	%%=============================================================%%

	%% \section{Example of another appendix section}\label{secA2}%
	%% Appendices may be used for helpful, supporting or essential material that would otherwise
	%% clutter, break up or be distracting to the text. Appendices can consist of sections, figures,
	%% tables and equations etc.

	\section{Assets}\label{appsec:assets}
	\subsection{Datasets}
	For pre-training we use the HG38 human reference genome~\cite{genome2009genome}.
	The Genomics Benchmark comes from \citet{grevsova2023genomic}.

	\subsection{Software and Libraries}
	In \ref{tab:assets}, we enumerate the relevant open-source software, and corresponding licenses, used in this work.

	\begin{table}[h]
		\caption{Open source libraries (and corresponding licenses) used in this work.}
		\label{tab:assets}
		\vskip 0.15in
		\begin{center}
			\begin{small}
				\begin{tabular}{ll}
					\toprule
					Library                                                & License                                                                            \\ \midrule
					GenomicsBenchmark~\cite{grevsova2023genomic}           & Apache 2.0                                                                         \\
					Mamba~\cite{gu2023mamba}                               & Apache 2.0                                                                         \\
					HuggingFace~\cite{wolf2019huggingface}                 & Apache 2.0                                                                         \\
					Hydra~\cite{Yadan2019Hydra}                            & MIT                                                                                \\
					HyenaDNA~\cite{nguyen2023hyenadna}                     & Apache 2.0                                                                         \\
					NumPy~\cite{harris2020array}                           & \href{https://numpy.org/doc/stable/license.html}{NumPy license}                    \\
					Matplotlib~\cite{Hunter2007}                           & \href{https://matplotlib.org/stable/users/project/license.html}{Matplotib license} \\
					OmegaConf                                              & BSD 3-Clause                                                                       \\
					Pandas \cite{reback2020pandas}                         & BSD 3-Clause ``New" or ``Revised"                                                  \\
					PyTorch~\cite{Paszke_PyTorch_An_Imperative_2019}       & BSD-3 Clause                                                                       \\
					PyTorch Lightning~\cite{Falcon_PyTorch_Lightning_2019} & Apache 2.0                                                                         \\
					Seaborn~\cite{Waskom2021}                              & BSD 3-Clause ``New" or ``Revised"                                                  \\ \bottomrule
				\end{tabular}
			\end{small}
		\end{center}
		% \vskip -0.1in
	\end{table}

	\section{Computational resources}

	Model training and inference were run on \glspl{gpu} with machine type varying by model size during pre-training and downstream tasks.
	We use two  A100 \glspl{gpu}.
\end{appendices}

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

\bibliography{sn-bibliography}% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl

\end{document}
